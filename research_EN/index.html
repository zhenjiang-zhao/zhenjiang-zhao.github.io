<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="Uujcslgie_ONEZttItwnET1CGh0CUrHMvCzctIv1Les"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Research | Zhenjiang Zhao</title> <meta name="author" content="Zhenjiang Zhao"> <meta name="description" content="Zhenjiang Zhao's page "> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://zhenjiang-zhao.github.io/research_EN/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Zhenjiang </span>Zhao</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/research_EN/">Research<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv_EN/">CV</a> </li> <li class="nav-item dropdown"> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">言語</a> <div class="dropdown-menu dropdown-menu-right" style="text-align:center;" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/research_JP">日本語</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/research_EN">English</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Research</h1> <p class="post-description"></p> </header> <article> <h3 id="publications"><strong>Publications</strong></h3> <div class="publications"> <h2 class="year">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ASE</abbr></div> <div id="Zhao+ASE24" class="col-sm-8"> <div class="title">Approximation-guided Fairness Testing through Discriminatory Space Analysis</div> <div class="author"> <em>Zhenjiang Zhao</em>, Takahisa Toda, and Takashi Kitamura</div> <div class="periodical"> <em>In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3691620.3695481" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/toda-lab/AFT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>As machine learning (ML) systems are increasingly used in various fields, including tasks with high social impact, concerns about their fairness are growing. To address these concerns, individual fairness testing (IFT) has been introduced to identify individual discriminatory instances (IDIs) that indicate the violation of individual fairness in a given ML classifier. In this paper, we propose a black-box testing algorithm for IFT, named Aft (short for Approximation-guided Fairness Testing). Aft constructs approximate models based on decision trees, and generates test cases by sampling paths of the decision trees. Our evaluation by experiments confirms that Aft outperforms the state-of-the-art black-box IFT algorithm ExpGA both in efficiency (by 3.42 times) and diversity of IDIs identified by algorithms (by 1.16 times).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">JSAI</abbr></div> <div id="Zhao+JSAI24" class="col-sm-8"> <div class="title">Toward Individual Fairness Testing for XGBoost Classifier through Formal Verification</div> <div class="author"> <em>Zhenjiang Zhao</em>, Takahisa Toda, and Takashi Kitamura</div> <div class="periodical"> <em>Proceedings of the Annual Conference of JSAI</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.jstage.jst.go.jp/article/pjsai/JSAI2024/0/JSAI2024_2L6OS19b04/_article/-char/en" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>There are growing concerns regarding the fairness of Machine Learning (ML) algorithms. Individual fairness testing is introduced to address the fairness concerns, and it aims to detect discriminatory instances which exhibit unfairness in a given classifier from its input space. XGBoost is one of the most prominent ML algorithms in recent years. In this study, we propose an individual fairness testing method for XGBoost classifier, leveraging the formal verification technique. To evaluate our method, we build XGBoost classifiers on three real-world datasets, and conduct individual fairness testing against them. Through the evaluation, we observe that our method can correctly detect discriminatory instances in XGBoost classifiers within an acceptable running time. Among all testing tasks, the longest running time for detecting 100 discriminatory instances is 2656.4 seconds.</p> </div> </div> </div> </li> </ol> <h2 class="year">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IST</abbr></div> <div id="ZHAO2024107390" class="col-sm-8"> <div class="title">Diversity-aware fairness testing of machine learning classifiers through hashing-based sampling</div> <div class="author"> <em>Zhenjiang Zhao</em>, Takahisa Toda, and Takashi Kitamura</div> <div class="periodical"> <em>Information and Software Technology</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S0950584923002458" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/toda-lab/Vbt-X" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Context: There are growing concerns about algorithmic fairness, as some machine learning (ML)-based algorithms have been found to exhibit biases against protected attributes such as gender, race, age and so on. Individual fairness requires an ML classifier to produce similar outputs for similar individuals. Verification Based Testing (Vbt) is a state-of-the-art black-box testing algorithm for individual fairness that leverages constraint solving to generate test cases. Objective: Generating diverse test cases is expected to facilitate efficient detection of diverse discriminatory data instances (i.e., cases that violate individual fairness). Hashing-based sampling techniques draw a sample approximately uniformly at random from the set of solutions of given Boolean constraints. We propose Vbt-X, which improves Vbt with hashing-based sampling, aiming to improve its testing performance. Method: We realize hashing-based sampling for Vbt. The challenge is that the off-the-shelf hashing-based sampling techniques cannot be integrated in a straightforward manner because the constraints in Vbt are generally not Boolean. Moreover, we propose several enhancement techniques to make Vbt-X more efficient. Results: To evaluate our method, we conduct experiments, where Vbt-X is compared to Vbt, Sg and ExpGA (other well-known fairness testing algorithms) over a set of configurations consisting of several datasets, protected attributes, and ML classifiers. The results show that, with each configuration, Vbt-X detects more discriminatory data instances with higher diversity than Vbt and Sg. Vbt-X detects discriminatory data instances with higher diversity than ExpGA, though the number of discriminatory data instances detected by Vbt-X is lesser than ExpGA. Conclusion: Our proposed method performs better than other state-of-the-art black-box fairness testing algorithms, particularly in terms of diversity. Our method can serve to efficiently identify flaws in ML classifiers with respect to individual fairness for subsequent improvements of an ML classifier. On the other hand, although our method is specific to individual fairness, it could work for testing other aspects of a software system such as security and counterfactual explanations with some technical adaptations, which remains for future work.</p> </div> </div> </div> </li></ol> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">SSBSE</abbr></div> <div id="Zhao+SSBSE22" class="col-sm-8"> <div class="title">Efficient Fairness Testing Through Hash-Based Sampling</div> <div class="author"> <em>Zhenjiang Zhao</em>, Takahisa Toda, and Takashi Kitamura</div> <div class="periodical"> <em>In Proceedings of Search-Based Software Engineering</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.springer.com/chapter/10.1007/978-3-031-21251-2_3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>There is a growing concern on algorithm fairness, according to wider adoption of machine learning techniques in our daily life. Testing of individual fairness is an approach to algorithm fairness concern. Verification Based Testing (VBT) is a state-of-the-art testing technique for individual fairness, that leverages verification techniques using constraint solving. In this paper, we develop a black-box individual fairness testing technique Vbt-X, which applies hash-based sampling techniques to the test case generation part of Vbt, aiming to improve its testing ability. Our evaluation by experiments confirms that Vbt-X improves the testing ability of Vbt by 2.92 times in average.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">SSBSE</abbr></div> <div id="Kitamura+SSBSE22" class="col-sm-8"> <div class="title">Applying Combinatorial Testing to Verification-Based Fairness Testing</div> <div class="author"> Takashi Kitamura, <em>Zhenjiang Zhao</em>, and Takahisa Toda</div> <div class="periodical"> <em>In Proceedings of Search-Based Software Engineering</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.springer.com/chapter/10.1007/978-3-031-21251-2_7" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Fairness testing, given a machine learning classifier, detects discriminatory data contained in it via executing test cases. In this paper, we propose a new approach to fairness testing named Vbt-Ct, which applies combinatorial t-way testing (CT) to Verification Based Testing (Vbt). Vbt is a state-of-the-art fairness testing method, which represents a given classifier under test in logical constraints and searches for test cases by solving such constraints. CT is a coverage-based sampling technique, with an ability to sample diverse test data from a search space specified by logical constraints. We implement a proof-of-concept of Vbt-Ct, and see its feasibility by experiments. We also discuss its advantages, current limitations, and further research directions.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">JSAI</abbr></div> <div id="Zhao+JSAI22" class="col-sm-8"> <div class="title">Note on CDCL Inference with Similar Learnt Clauses (in Japanese)</div> <div class="author"> <em>Zhenjiang Zhao</em>, and Takahisa Toda</div> <div class="periodical"> <em>Proceedings of the Annual Conference of JSAI</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.jstage.jst.go.jp/article/pjsai/JSAI2022/0/JSAI2022_4K1GS105/_article/-char/en" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The conflict-driven clause learning (CDCL) is a standard algorithmic framework on which almost state-of-the-art SAT solvers are based. During the solving process of the CDCL solver, many learnt clauses are generated, and those turned out to be useless in terms of some criteria are removed. Since the decision commonly relies on heuristics, the same clauses can appear multiple times. Hence, the evaluation of the learnt clause utility has a significant impact on the solver’s performance. The recently proposed DL heuristic determines the utility in terms of the number of times clauses are generated. To improve this, we introduce the similarity of learnt clauses and propose a similarity-based clause management method. In experiments we compared our method with the DL, both implemented on top of CaDiCal, and we confirmed that our method outperforms the DL as well as the intact CaDiCal in both PAR-2 scores and the number of solved instances.</p> </div> </div> </div> </li> </ol> </div> <h3 id="oral--poster-presentations"><strong>Oral &amp; Poster Presentations</strong></h3> <div class="publications"> <h2 class="year">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">SES</abbr></div> <div id="Zhao+SES25" class="col-sm-8"> <div class="title">Presentation at IPSJ/SIGSE Software Engineering Symposium (SES2025), Invited for The Paper "Approximation-guided Fairness Testing through Discriminatory Space Analysis" Published in ASE</div> <div class="author"> <em>Zhenjiang Zhao</em> </div> <div class="periodical"> <em>In IPSJ/SIGSE Software Engineering Symposium,</em> Sep 2025 </div> <div class="links"> <a href="https://ses.sigse.jp/2025/program.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">FIT</abbr></div> <div id="Zhao+FIT25" class="col-sm-8"> <div class="title">Presentation in the Top Conference Section at Forum on Information Technology, Invited for The Paper "Approximation-guided Fairness Testing through Discriminatory Space Analysis" Published in ASE</div> <div class="author"> <em>Zhenjiang Zhao</em> </div> <div class="periodical"> <em>In Forum on Information Technology,</em> Sep 2025 </div> <div class="links"> <a href="https://www.ipsj.or.jp/event/fit/fit2025/abstract/data/html/event/TCS7-2.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">MLSE</abbr></div> <div id="Zhao+mlse25" class="col-sm-8"> <div class="title">Consideration on Quantitative Verification of Fairness in Machine Learning Models</div> <div class="author"> <em>Zhenjiang Zhao</em>, Takahisa Toda, and Takashi Kitamura</div> <div class="periodical"> <em>In Special Interest Group on Machine Learning Systems Engineering,</em> Jul 2025 </div> <div class="links"> <a href="https://mlxse.connpass.com/event/351050/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> </ol> <h2 class="year">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">FOSE</abbr></div> <div id="Zhao+FOSE24" class="col-sm-8"> <div class="title">Considerations on the Approximate Performance of Verification-Based Fairness Testing Techniques</div> <div class="author"> <em>Zhenjiang Zhao</em>, Takahisa Toda, and Takashi Kitamura</div> <div class="periodical"> <em>In Workshop of Fundamentals of Software Engineering,</em> Nov 2024 </div> <div class="links"> <a href="https://fose.jssst.or.jp/fose2024/program.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IBIS</abbr></div> <div id="Zhao+IBIS24" class="col-sm-8"> <div class="title">Consideration of a Verification-Based Fairness Testing Technique Without Constraint Solving</div> <div class="author"> <em>Zhenjiang Zhao</em>, Takahisa Toda, and Takashi Kitamura</div> <div class="periodical"> <em>In Workshop of Information-Based Induction Sciences,</em> Nov 2024 </div> <div class="links"> <a href="https://ibisml.org/ibis2024/posters/#s" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">SES</abbr></div> <div id="Zhao+SES24" class="col-sm-8"> <div class="title">Presentation at IPSJ/SIGSE Software Engineering Symposium (SES2024), Invited for The Paper "Diversity-aware Fairness Testing of Machine Learning Classifiers through Hashing-based Sampling" Published in IST</div> <div class="author"> <em>Zhenjiang Zhao</em> </div> <div class="periodical"> <em>In IPSJ/SIGSE Software Engineering Symposium,</em> Sep 2024 </div> <div class="links"> <a href="https://ses.sigse.jp/2024/program.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">SES-WS</abbr></div> <div id="Zhao+SES-WS524" class="col-sm-8"> <div class="title">Trends in Techniques for Individual Fairness Testing of Machine Learning Models</div> <div class="author"> <em>Zhenjiang Zhao</em> </div> <div class="periodical"> <em>In Workshop on Algorithmic Fairness and Software Engineering at IPSJ/SIGSE Software Engineering Symposium,</em> Sep 2024 </div> <div class="links"> <a href="https://ses.sigse.jp/2024/workshop.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">FIT</abbr></div> <div id="Zhao+FIT24" class="col-sm-8"> <div class="title">Presentation in the Top Conference Section at Forum on Information Technology, Invited for The Paper "Diversity-aware Fairness Testing of Machine Learning Classifiers through Hashing-based Sampling" Published in IST</div> <div class="author"> <em>Zhenjiang Zhao</em> </div> <div class="periodical"> <em>In Forum on Information Technology,</em> Sep 2024 </div> <div class="links"> <a href="https://www.ipsj.or.jp/event/fit/fit2024/abstract/data/html/event/event_TCS2-3.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> </ol> <h2 class="year">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">MLSE</abbr></div> <div id="Zhao+mlse23" class="col-sm-8"> <div class="title">Consideration of Fairness Testing Method Based on a Complete Search for Paths in Decision Tree</div> <div class="author"> <em>Zhenjiang Zhao</em>, Takahisa Toda, and Takashi Kitamura</div> <div class="periodical"> <em>In Special Interest Group on Machine Learning Systems Engineering,</em> Jun 2023 </div> <div class="links"> <a href="https://mlxse.connpass.com/event/284014/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">FOSE</abbr></div> <div id="Zhao+FOSE23" class="col-sm-8"> <div class="title">A Diversity-Aware Fairness Testing Technique and Considerations of Its Diversity</div> <div class="author"> <em>Zhenjiang Zhao</em>, Takahisa Toda, and Takashi Kitamura</div> <div class="periodical"> <em>In Workshop of Fundamentals of Software Engineering,</em> Nov 2023 </div> <div class="links"> <a href="https://fose.jssst.or.jp/fose2023/program.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">PROSYM</abbr></div> <div id="Zhao+prosym23" class="col-sm-8"> <div class="title">Fairness Testing of Machine Learning Model</div> <div class="author"> <em>Zhenjiang Zhao</em>, Takahisa Toda, and Takashi Kitamura</div> <div class="periodical"> <em>In Programming Symposium, Information Processing Society of Japan,</em> Jan 2023 </div> <div class="links"> <a href="https://prosym.org/64/program.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Zhao+sigse23" class="col-sm-8"> <div class="title">Paper Introduction: Efficient Fairness Testing Through Hash-Based Sampling (SSBSE2022)</div> <div class="author"> <em>Zhenjiang Zhao</em> </div> <div class="periodical"> <em>In IPSJ/SIGSE Winter Workshop,</em> Jan 2023 </div> <div class="links"> <a href="https://wws.sigse.jp/2023/program.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IBIS</abbr></div> <div id="Zhao+IBIS22" class="col-sm-8"> <div class="title">Fairness Testing Method ’VBT-X‘ and Its Future Challenges</div> <div class="author"> <em>Zhenjiang Zhao</em>, Takahisa Toda, and Takashi Kitamura</div> <div class="periodical"> <em>In Workshop of Information-Based Induction Sciences,</em> Nov 2022 </div> <div class="links"> <a href="https://ibisml.org/ibis2022/posters/#s" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">FOSE</abbr></div> <div id="Zhao+FOSE22" class="col-sm-8"> <div class="title">VBT-X: A Fairness Testing Method of Machine Learning Model</div> <div class="author"> <em>Zhenjiang Zhao</em>, Takahisa Toda, and Takashi Kitamura</div> <div class="periodical"> <em>In Workshop of Fundamentals of Software Engineering,</em> Nov 2022 </div> <div class="links"> <a href="https://fose.jssst.or.jp/fose2022/program.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Zhenjiang Zhao. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>